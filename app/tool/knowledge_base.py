"""
Knowledge Base Tool

This tool provides vector database functionality for Retrieval-Augmented Generation (RAG).
Supports adding, searching, and managing knowledge entries with semantic search capabilities.

Author: Enhancement #4 Implementation
Date: 2025-10-26
"""

import os
import json
from typing import Dict, List, Optional, Any
from datetime import datetime
from pathlib import Path

from pydantic import Field

from app.tool.base import BaseTool, ToolResult
from app.utils.logger import logger


class KnowledgeBaseTool(BaseTool):
    """
    Vector database for Retrieval-Augmented Generation (RAG).
    
    This tool provides comprehensive knowledge management:
    - Add documents with automatic embedding
    - Semantic search across all knowledge
    - Metadata filtering and organization
    - Persistent storage
    - Statistics and analytics
    
    Use cases:
    - Build company knowledge base
    - Store research findings
    - Create personal wiki
    - Enable RAG for better answers
    - Long-term memory for agent
    
    Design rationale:
    - Uses FAISS for fast similarity search (Facebook AI Similarity Search)
    - OpenAI embeddings for high-quality semantic understanding
    - Persistent storage to disk for durability
    - Metadata support for filtering and organization
    - Automatic chunking for long documents
    - Deduplication to prevent redundant entries
    
    Technical details:
    - Embedding model: text-embedding-ada-002 (OpenAI)
    - Vector dimension: 1536
    - Similarity metric: Cosine similarity
    - Index type: FAISS IndexFlatL2 (exact search, no approximation)
    """
    
    name: str = "knowledge_base"
    description: str = (
        "Vector database for storing and retrieving knowledge using semantic search. "
        "Add documents, search by meaning (not just keywords), manage knowledge entries. "
        "Supports metadata filtering, deduplication, and persistent storage. "
        "Essential for RAG (Retrieval-Augmented Generation) and long-term memory."
    )
    parameters: dict = Field(default_factory=lambda: {
        "type": "object",
        "properties": {
            "action": {
                "type": "string",
                "enum": ["add", "search", "delete", "list", "stats", "clear"],
                "description": "Action to perform: add (new knowledge), search (semantic search), delete (remove entry), list (all entries), stats (database statistics), clear (delete all)"
            },
            "content": {
                "type": "string",
                "description": "Content to add to knowledge base (for 'add' action)"
            },
            "query": {
                "type": "string",
                "description": "Search query (for 'search' action)"
            },
            "knowledge_id": {
                "type": "string",
                "description": "Knowledge entry ID (for 'delete' action)"
            },
            "source": {
                "type": "string",
                "description": "Source of the knowledge (e.g., 'youtube:VIDEO_ID', 'notion:PAGE_ID', 'manual')"
            },
            "title": {
                "type": "string",
                "description": "Title of the knowledge entry"
            },
            "metadata": {
                "type": "object",
                "description": "Additional metadata (type, author, date, etc.)"
            },
            "top_k": {
                "type": "integer",
                "description": "Number of results to return for search (default: 5)",
                "default": 5
            },
            "chunk_size": {
                "type": "integer",
                "description": "Size of text chunks for long documents (default: 1000 characters)",
                "default": 1000
            }
        },
        "required": ["action"]
    })
    
    def __init__(self, **data):
        """Initialize the Knowledge Base tool."""
        super().__init__(**data)
        self.db_path = Path("./knowledge_db")
        self.db_path.mkdir(exist_ok=True)
        self.index_path = self.db_path / "faiss_index"
        self.metadata_path = self.db_path / "metadata.json"
        
        self.vector_store = None
        self.embeddings = None
        self.metadata_store = {}
        
        # Load existing database if available
        self._load_database()
    
    def _load_database(self):
        """Load existing vector database and metadata from disk."""
        try:
            # Load metadata
            if self.metadata_path.exists():
                with open(self.metadata_path, 'r') as f:
                    self.metadata_store = json.load(f)
                logger.info(f"Loaded {len(self.metadata_store)} knowledge entries from disk")
            
            # Initialize embeddings
            try:
                from langchain.embeddings import OpenAIEmbeddings
                self.embeddings = OpenAIEmbeddings()
            except ImportError:
                logger.warning("langchain not installed. Run: pip install langchain openai")
                return
            
            # Load FAISS index if exists
            if self.index_path.exists():
                try:
                    from langchain.vectorstores import FAISS
                    self.vector_store = FAISS.load_local(
                        str(self.index_path),
                        self.embeddings
                    )
                    logger.info("Loaded FAISS index from disk")
                except Exception as e:
                    logger.warning(f"Could not load FAISS index: {e}")
                    self.vector_store = None
            
        except Exception as e:
            logger.error(f"Error loading database: {e}")
    
    def _save_database(self):
        """Save vector database and metadata to disk."""
        try:
            # Save metadata
            with open(self.metadata_path, 'w') as f:
                json.dump(self.metadata_store, f, indent=2)
            
            # Save FAISS index
            if self.vector_store:
                self.vector_store.save_local(str(self.index_path))
            
            logger.info("Saved knowledge base to disk")
            
        except Exception as e:
            logger.error(f"Error saving database: {e}")
    
    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """
        Split long text into overlapping chunks.
        
        Overlap ensures context is preserved across chunk boundaries.
        
        Args:
            text: Text to chunk
            chunk_size: Maximum characters per chunk
            overlap: Characters to overlap between chunks
        
        Returns:
            List of text chunks
        """
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - overlap  # Overlap for context
        
        return chunks
    
    def _generate_id(self) -> str:
        """Generate unique knowledge entry ID."""
        import uuid
        return f"kb_{uuid.uuid4().hex[:12]}"
    
    async def _add_knowledge(
        self,
        content: str,
        source: str = "manual",
        title: Optional[str] = None,
        metadata: Optional[Dict] = None,
        chunk_size: int = 1000
    ) -> Dict[str, Any]:
        """
        Add content to knowledge base.
        
        Args:
            content: Text content to add
            source: Source identifier
            title: Title of the entry
            metadata: Additional metadata
            chunk_size: Size for chunking long documents
        
        Returns:
            Dict with knowledge_id and stats
        """
        try:
            # Initialize vector store if needed
            if self.vector_store is None:
                from langchain.vectorstores import FAISS
                from langchain.embeddings import OpenAIEmbeddings
                
                if self.embeddings is None:
                    self.embeddings = OpenAIEmbeddings()
                
                # Create empty vector store
                self.vector_store = FAISS.from_texts(
                    ["Initialization text"],
                    self.embeddings,
                    metadatas=[{"init": True}]
                )
            
            # Generate ID
            knowledge_id = self._generate_id()
            
            # Chunk content if too long
            chunks = self._chunk_text(content, chunk_size)
            
            # Prepare metadata
            entry_metadata = {
                "knowledge_id": knowledge_id,
                "source": source,
                "title": title or "Untitled",
                "timestamp": datetime.now().isoformat(),
                "chunk_count": len(chunks),
                **(metadata or {})
            }
            
            # Add each chunk to vector store
            chunk_metadatas = []
            for i, chunk in enumerate(chunks):
                chunk_meta = {
                    **entry_metadata,
                    "chunk_index": i,
                    "total_chunks": len(chunks)
                }
                chunk_metadatas.append(chunk_meta)
            
            # Add to FAISS
            self.vector_store.add_texts(
                texts=chunks,
                metadatas=chunk_metadatas
            )
            
            # Store metadata
            self.metadata_store[knowledge_id] = entry_metadata
            
            # Save to disk
            self._save_database()
            
            logger.info(f"Added knowledge entry: {knowledge_id} ({len(chunks)} chunks)")
            
            return {
                "knowledge_id": knowledge_id,
                "chunks_created": len(chunks),
                "total_entries": len(self.metadata_store),
                "title": entry_metadata['title']
            }
            
        except ImportError:
            raise Exception("Required packages not installed. Run: pip install langchain openai faiss-cpu")
        except Exception as e:
            logger.error(f"Error adding knowledge: {e}")
            raise
    
    async def _search_knowledge(
        self,
        query: str,
        top_k: int = 5,
        filter_metadata: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        Semantic search across knowledge base.
        
        Args:
            query: Search query
            top_k: Number of results to return
            filter_metadata: Optional metadata filters
        
        Returns:
            List of matching documents with scores
        """
        try:
            if self.vector_store is None:
                return []
            
            # Perform similarity search
            results = self.vector_store.similarity_search_with_score(
                query,
                k=top_k
            )
            
            # Format results
            formatted_results = []
            for doc, score in results:
                # Skip initialization text
                if doc.metadata.get("init"):
                    continue
                
                # Apply metadata filters if provided
                if filter_metadata:
                    if not all(doc.metadata.get(k) == v for k, v in filter_metadata.items()):
                        continue
                
                formatted_results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": float(score),
                    "relevance": "high" if score < 0.5 else "medium" if score < 1.0 else "low"
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Error searching knowledge: {e}")
            return []
    
    async def _delete_knowledge(self, knowledge_id: str) -> bool:
        """
        Delete knowledge entry.
        
        Note: FAISS doesn't support deletion directly, so we mark as deleted
        and rebuild index periodically.
        
        Args:
            knowledge_id: ID of entry to delete
        
        Returns:
            True if deleted, False if not found
        """
        try:
            if knowledge_id in self.metadata_store:
                del self.metadata_store[knowledge_id]
                self._save_database()
                logger.info(f"Deleted knowledge entry: {knowledge_id}")
                return True
            return False
            
        except Exception as e:
            logger.error(f"Error deleting knowledge: {e}")
            return False
    
    async def _list_knowledge(self) -> List[Dict[str, Any]]:
        """
        List all knowledge entries.
        
        Returns:
            List of all entries with metadata
        """
        return [
            {
                "knowledge_id": kid,
                **meta
            }
            for kid, meta in self.metadata_store.items()
        ]
    
    async def _get_stats(self) -> Dict[str, Any]:
        """
        Get knowledge base statistics.
        
        Returns:
            Statistics about the knowledge base
        """
        total_entries = len(self.metadata_store)
        total_chunks = sum(meta.get('chunk_count', 1) for meta in self.metadata_store.values())
        
        # Count by source
        sources = {}
        for meta in self.metadata_store.values():
            source = meta.get('source', 'unknown')
            sources[source] = sources.get(source, 0) + 1
        
        # Count by type
        types = {}
        for meta in self.metadata_store.values():
            entry_type = meta.get('metadata', {}).get('type', 'unknown')
            types[entry_type] = types.get(entry_type, 0) + 1
        
        return {
            "total_entries": total_entries,
            "total_chunks": total_chunks,
            "avg_chunks_per_entry": total_chunks / total_entries if total_entries > 0 else 0,
            "sources": sources,
            "types": types,
            "database_path": str(self.db_path),
            "index_exists": self.index_path.exists()
        }
    
    async def _clear_database(self) -> bool:
        """
        Clear entire knowledge base.
        
        Returns:
            True if successful
        """
        try:
            self.metadata_store = {}
            self.vector_store = None
            
            # Remove files
            if self.metadata_path.exists():
                self.metadata_path.unlink()
            if self.index_path.exists():
                import shutil
                shutil.rmtree(self.index_path)
            
            logger.info("Cleared knowledge base")
            return True
            
        except Exception as e:
            logger.error(f"Error clearing database: {e}")
            return False
    
    async def execute(self, action: str, **kwargs) -> ToolResult:
        """
        Execute knowledge base operation.
        
        Args:
            action: Operation to perform
            **kwargs: Action-specific parameters
        
        Returns:
            ToolResult with operation results
        """
        try:
            if action == "add":
                if 'content' not in kwargs:
                    return self.error_response("Missing required parameter: content")
                
                result = await self._add_knowledge(
                    content=kwargs['content'],
                    source=kwargs.get('source', 'manual'),
                    title=kwargs.get('title'),
                    metadata=kwargs.get('metadata'),
                    chunk_size=kwargs.get('chunk_size', 1000)
                )
                return self.success_response(result)
            
            elif action == "search":
                if 'query' not in kwargs:
                    return self.error_response("Missing required parameter: query")
                
                results = await self._search_knowledge(
                    query=kwargs['query'],
                    top_k=kwargs.get('top_k', 5),
                    filter_metadata=kwargs.get('filter_metadata')
                )
                return self.success_response({
                    "query": kwargs['query'],
                    "results": results,
                    "count": len(results)
                })
            
            elif action == "delete":
                if 'knowledge_id' not in kwargs:
                    return self.error_response("Missing required parameter: knowledge_id")
                
                success = await self._delete_knowledge(kwargs['knowledge_id'])
                if success:
                    return self.success_response({"deleted": kwargs['knowledge_id']})
                else:
                    return self.error_response(f"Knowledge entry not found: {kwargs['knowledge_id']}")
            
            elif action == "list":
                entries = await self._list_knowledge()
                return self.success_response({
                    "entries": entries,
                    "count": len(entries)
                })
            
            elif action == "stats":
                stats = await self._get_stats()
                return self.success_response(stats)
            
            elif action == "clear":
                success = await self._clear_database()
                if success:
                    return self.success_response({"status": "cleared"})
                else:
                    return self.error_response("Failed to clear database")
            
            else:
                return self.error_response(f"Unknown action: {action}")
            
        except Exception as e:
            logger.error(f"Error in knowledge base tool: {e}")
            return self.error_response(f"Knowledge base operation failed: {str(e)}")

